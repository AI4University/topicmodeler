{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319e2032",
   "metadata": {},
   "source": [
    "# Lemmatize CORDIS projects using Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ccc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ee1f5",
   "metadata": {},
   "source": [
    "## 1. Read project information and concatenate the `title` and `objective` fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f6fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=====================================================>(987 + 8) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects before language filtering: 61163\n",
      "CPU times: user 25.7 ms, sys: 5.49 ms, total: 31.2 ms\n",
      "Wall time: 1min 1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Loading papers table text fields, and concatenating them for lemmatization\n",
    "#projects = spark.sql(\"SELECT projectID, title, objective FROM parquet.`/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet/projects.parquet`\")\n",
    "projects = spark.read.parquet('/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet/projects.parquet')\n",
    "projects = projects.repartition(numPartitions=1000)\n",
    "\n",
    "#Concatenate text fields to lemmatize\n",
    "projects = (\n",
    "    projects.withColumn(\"rawtext\",F.concat_ws('. ', \"title\", \"objective\"))\n",
    "    .select(\"projectID\",\"rawtext\")\n",
    ")\n",
    "\n",
    "print('Number of projects before language filtering:', projects.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba78d8",
   "metadata": {},
   "source": [
    "## 2. Filter project abstracts that are not in English Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba2ecc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "[ | ]ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====>                                                     (1 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ â€” ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 15:47:48.473516: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-04 15:47:48.554100: I external/org_tensorflow/tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2399870000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/04 15:47:54 ERROR scheduler.TaskSchedulerImpl: Lost executor 2 on node07.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:54 WARN scheduler.TaskSetManager: Lost task 19.0 in stage 7.0 (TID 1068) (node07.cluster.tsc.uc3m.es executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:54 ERROR scheduler.TaskSchedulerImpl: Lost executor 0 on node01.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:54 WARN scheduler.TaskSetManager: Lost task 26.0 in stage 7.0 (TID 1067) (node01.cluster.tsc.uc3m.es executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:54 WARN scheduler.TaskSetManager: Lost task 17.0 in stage 7.0 (TID 1087) (node01.cluster.tsc.uc3m.es executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:54 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 7.0 (TID 1078) (node01.cluster.tsc.uc3m.es executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:54 WARN scheduler.TaskSetManager: Lost task 27.0 in stage 7.0 (TID 1095) (node01.cluster.tsc.uc3m.es executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:54 ERROR scheduler.TaskSchedulerImpl: Lost executor 8 on node08.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:54 WARN scheduler.TaskSetManager: Lost task 13.0 in stage 7.0 (TID 1070) (node08.cluster.tsc.uc3m.es executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:54 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 7.0 (TID 1073) (node08.cluster.tsc.uc3m.es executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:54 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 7.0 (TID 1082) (node08.cluster.tsc.uc3m.es executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:54 WARN scheduler.TaskSetManager: Lost task 4.0 in stage 7.0 (TID 1069) (node08.cluster.tsc.uc3m.es executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:56 ERROR scheduler.TaskSchedulerImpl: Lost executor 5 on node72.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:56 WARN scheduler.TaskSetManager: Lost task 8.0 in stage 7.0 (TID 1079) (node72.cluster.tsc.uc3m.es executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:56 WARN scheduler.TaskSetManager: Lost task 18.0 in stage 7.0 (TID 1088) (node72.cluster.tsc.uc3m.es executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:56 WARN scheduler.TaskSetManager: Lost task 28.0 in stage 7.0 (TID 1096) (node72.cluster.tsc.uc3m.es executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:56 WARN scheduler.TaskSetManager: Lost task 35.0 in stage 7.0 (TID 1103) (node72.cluster.tsc.uc3m.es executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:57 ERROR scheduler.TaskSchedulerImpl: Lost executor 3 on node64.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:57 WARN scheduler.TaskSetManager: Lost task 32.0 in stage 7.0 (TID 1100) (node64.cluster.tsc.uc3m.es executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:57 WARN scheduler.TaskSetManager: Lost task 3.0 in stage 7.0 (TID 1075) (node64.cluster.tsc.uc3m.es executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:57 WARN scheduler.TaskSetManager: Lost task 14.0 in stage 7.0 (TID 1084) (node64.cluster.tsc.uc3m.es executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:57 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 7.0 (TID 1092) (node64.cluster.tsc.uc3m.es executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 ERROR scheduler.TaskSchedulerImpl: Lost executor 4 on node65.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 10.0 in stage 7.0 (TID 1081) (node65.cluster.tsc.uc3m.es executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 7.0 (TID 1072) (node65.cluster.tsc.uc3m.es executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 21.0 in stage 7.0 (TID 1090) (node65.cluster.tsc.uc3m.es executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 30.0 in stage 7.0 (TID 1098) (node65.cluster.tsc.uc3m.es executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 ERROR scheduler.TaskSchedulerImpl: Lost executor 7 on node09.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 25.0 in stage 7.0 (TID 1094) (node09.cluster.tsc.uc3m.es executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 34.0 in stage 7.0 (TID 1102) (node09.cluster.tsc.uc3m.es executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 7.0 (TID 1086) (node09.cluster.tsc.uc3m.es executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 7.0 (TID 1077) (node09.cluster.tsc.uc3m.es executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/04 15:47:58 ERROR scheduler.TaskSchedulerImpl: Lost executor 6 on node70.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 29.0 in stage 7.0 (TID 1097) (node70.cluster.tsc.uc3m.es executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 9.0 in stage 7.0 (TID 1080) (node70.cluster.tsc.uc3m.es executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 20.0 in stage 7.0 (TID 1089) (node70.cluster.tsc.uc3m.es executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:58 WARN scheduler.TaskSetManager: Lost task 19.1 in stage 7.0 (TID 1071) (node70.cluster.tsc.uc3m.es executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:59 ERROR scheduler.TaskSchedulerImpl: Lost executor 1 on node46.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:59 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 7.0 (TID 1076) (node46.cluster.tsc.uc3m.es executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:59 WARN scheduler.TaskSetManager: Lost task 15.0 in stage 7.0 (TID 1085) (node46.cluster.tsc.uc3m.es executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:59 WARN scheduler.TaskSetManager: Lost task 24.0 in stage 7.0 (TID 1093) (node46.cluster.tsc.uc3m.es executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:59 WARN scheduler.TaskSetManager: Lost task 33.0 in stage 7.0 (TID 1101) (node46.cluster.tsc.uc3m.es executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:59 ERROR scheduler.TaskSchedulerImpl: Lost executor 9 on node43.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:59 WARN scheduler.TaskSetManager: Lost task 22.0 in stage 7.0 (TID 1091) (node43.cluster.tsc.uc3m.es executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:59 WARN scheduler.TaskSetManager: Lost task 31.0 in stage 7.0 (TID 1099) (node43.cluster.tsc.uc3m.es executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:59 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 7.0 (TID 1074) (node43.cluster.tsc.uc3m.es executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:47:59 WARN scheduler.TaskSetManager: Lost task 12.0 in stage 7.0 (TID 1083) (node43.cluster.tsc.uc3m.es executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:48:53 ERROR scheduler.TaskSchedulerImpl: Lost executor 17 on node72.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:48:53 WARN scheduler.TaskSetManager: Lost task 31.1 in stage 7.0 (TID 1106) (node72.cluster.tsc.uc3m.es executor 17): ExecutorLostFailure (executor 17 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:48:53 WARN scheduler.TaskSetManager: Lost task 2.1 in stage 7.0 (TID 1105) (node72.cluster.tsc.uc3m.es executor 17): ExecutorLostFailure (executor 17 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:48:53 WARN scheduler.TaskSetManager: Lost task 22.1 in stage 7.0 (TID 1107) (node72.cluster.tsc.uc3m.es executor 17): ExecutorLostFailure (executor 17 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:48:53 WARN scheduler.TaskSetManager: Lost task 12.1 in stage 7.0 (TID 1104) (node72.cluster.tsc.uc3m.es executor 17): ExecutorLostFailure (executor 17 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:48:53 ERROR scheduler.TaskSchedulerImpl: Lost executor 18 on node55.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:48:53 WARN scheduler.TaskSetManager: Lost task 24.1 in stage 7.0 (TID 1109) (node55.cluster.tsc.uc3m.es executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:48:53 WARN scheduler.TaskSetManager: Lost task 5.1 in stage 7.0 (TID 1111) (node55.cluster.tsc.uc3m.es executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:48:53 WARN scheduler.TaskSetManager: Lost task 33.1 in stage 7.0 (TID 1108) (node55.cluster.tsc.uc3m.es executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:48:53 WARN scheduler.TaskSetManager: Lost task 15.1 in stage 7.0 (TID 1110) (node55.cluster.tsc.uc3m.es executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/04 15:49:02 ERROR scheduler.TaskSchedulerImpl: Lost executor 10 on node72.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:02 WARN scheduler.TaskSetManager: Lost task 24.2 in stage 7.0 (TID 1115) (node72.cluster.tsc.uc3m.es executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:02 WARN scheduler.TaskSetManager: Lost task 5.2 in stage 7.0 (TID 1114) (node72.cluster.tsc.uc3m.es executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:02 WARN scheduler.TaskSetManager: Lost task 33.2 in stage 7.0 (TID 1113) (node72.cluster.tsc.uc3m.es executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:02 WARN scheduler.TaskSetManager: Lost task 15.2 in stage 7.0 (TID 1112) (node72.cluster.tsc.uc3m.es executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:07 ERROR scheduler.TaskSchedulerImpl: Lost executor 11 on node63.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:07 WARN scheduler.TaskSetManager: Lost task 2.2 in stage 7.0 (TID 1118) (node63.cluster.tsc.uc3m.es executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:07 WARN scheduler.TaskSetManager: Lost task 22.2 in stage 7.0 (TID 1117) (node63.cluster.tsc.uc3m.es executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:07 WARN scheduler.TaskSetManager: Lost task 12.2 in stage 7.0 (TID 1116) (node63.cluster.tsc.uc3m.es executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:07 WARN scheduler.TaskSetManager: Lost task 31.2 in stage 7.0 (TID 1119) (node63.cluster.tsc.uc3m.es executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:12 ERROR scheduler.TaskSchedulerImpl: Lost executor 13 on node64.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:12 WARN scheduler.TaskSetManager: Lost task 15.3 in stage 7.0 (TID 1120) (node64.cluster.tsc.uc3m.es executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:12 ERROR scheduler.TaskSetManager: Task 15 in stage 7.0 failed 4 times; aborting job\n",
      "22/05/04 15:49:12 WARN scheduler.TaskSetManager: Lost task 24.3 in stage 7.0 (TID 1123) (node64.cluster.tsc.uc3m.es executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:12 WARN scheduler.TaskSetManager: Lost task 5.3 in stage 7.0 (TID 1122) (node64.cluster.tsc.uc3m.es executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:12 WARN scheduler.TaskSetManager: Lost task 33.3 in stage 7.0 (TID 1121) (node64.cluster.tsc.uc3m.es executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/05/04 15:49:13 WARN scheduler.TaskSetManager: Lost task 31.3 in stage 7.0 (TID 1125) (node07.cluster.tsc.uc3m.es executor 19): TaskKilled (Stage cancelled)\n",
      "22/05/04 15:49:13 WARN scheduler.TaskSetManager: Lost task 22.3 in stage 7.0 (TID 1127) (node07.cluster.tsc.uc3m.es executor 19): TaskKilled (Stage cancelled)\n",
      "22/05/04 15:49:13 WARN scheduler.TaskSetManager: Lost task 19.2 in stage 7.0 (TID 1124) (node07.cluster.tsc.uc3m.es executor 19): TaskKilled (Stage cancelled)\n",
      "22/05/04 15:49:13 WARN scheduler.TaskSetManager: Lost task 12.3 in stage 7.0 (TID 1126) (node07.cluster.tsc.uc3m.es executor 19): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o152.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 7.0 failed 4 times, most recent failure: Lost task 15.3 in stage 7.0 (TID 1120) (node64.cluster.tsc.uc3m.es executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3006)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3005)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3005)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/opt/spark-3.1.1-bin-2.8.3/python/pyspark/sql/dataframe.py:664\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark-3.1.1-bin-2.8.3/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark-3.1.1-bin-2.8.3/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark-3.1.1-bin-2.8.3/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o152.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 7.0 failed 4 times, most recent failure: Lost task 15.3 in stage 7.0 (TID 1120) (node64.cluster.tsc.uc3m.es executor 13): ExecutorLostFailure (executor 13 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3006)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3005)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3005)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Pipeline for language detection\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"rawtext\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "languageDetector = LanguageDetectorDL.pretrained() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"language\")\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "      documentAssembler,\n",
    "      languageDetector\n",
    "    ])\n",
    "\n",
    "#Apply language detection pipeline\n",
    "projects = pipeline.fit(projects).transform(projects)\n",
    "projects = (\n",
    "    projects.filter(F.col(\"language.result\")[0]==\"en\")\n",
    "    .drop(\"language\")\n",
    ")\n",
    "\n",
    "print('Number of projects in English:', projects.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d59da",
   "metadata": {},
   "source": [
    "## 3. Define and Run Lemmatization Pipeline\n",
    "\n",
    "   - We work on documents created in Subsection 2\n",
    "   - Sentence Detection and Tokenizer applied to detect tokens\n",
    "   - Lemmatization is carried out\n",
    "   - Stopwords are applied\n",
    "   - Punctuation symbols are removed\n",
    "   - Result is converted back from Spark NLP annotations to string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eecf0f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 201711                                                                                                                   \n",
      " lemmas    | fuel cell stack assembly diagnostics gebze institute technology actively engage research promote hydrogen energy tech... \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 710473                                                                                                                   \n",
      " lemmas    | weather snow machine drive renewable energy sources climate change shorten ski season less snow precipitation high av... \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 673134                                                                                                                   \n",
      " lemmas    | novel ozone thermal shock conservation process vegetables fiordelisi leader production dehydrate vegetable italy firs... \n",
      "-RECORD 3-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 605292                                                                                                                   \n",
      " lemmas    | best class vehicle safe urban mobility sustainable transport valuechain behicle behicle project deliver safe lightwei... \n",
      "-RECORD 4-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 681884                                                                                                                   \n",
      " lemmas    | travel transculturality identity england c1550 1700 central research question project pose mobility great age travel ... \n",
      "-RECORD 5-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 746648                                                                                                                   \n",
      " lemmas    | synergistic resistive switching perovskite silicon carbide material advanced reram micro devices perovskite oxide one... \n",
      "-RECORD 6-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 838559                                                                                                                   \n",
      " lemmas    | new biology oncogenic pi 3kinase pi 3kinase pi3k signal regulate multiple cell function one frequently geneticallyact... \n",
      "-RECORD 7-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 806245                                                                                                                   \n",
      " lemmas    | retinal image prevention early detection chronic disease overall objective retinal initiative ulma embedded solutions... \n",
      "-RECORD 8-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 883371                                                                                                                   \n",
      " lemmas    | nextgeneration equipment tool missioncritical strategy first responders evolve threat climate change consequence indu... \n",
      "-RECORD 9-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 247605                                                                                                                   \n",
      " lemmas    | grail2 gnssbased atp system railway low density lines objective grail2 develop validate gnss base etcs prototypeappli... \n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 83.3 ms, sys: 36.2 ms, total: 120 ms\n",
      "Wall time: 16.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#Next, we carry out the lemmatization pipeline\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "stopWords = StopWordsCleaner() \\\n",
    "    .setInputCols([\"lemma\"]) \\\n",
    "    .setOutputCol(\"cleanlemma\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"cleanlemma\"]) \\\n",
    "    .setOutputCol(\"normalizedlemma\") \\\n",
    "    .setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"])\n",
    "\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['normalizedlemma'])\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "#      documentAssembler,\n",
    "      sentenceDetector,\n",
    "      tokenizer,\n",
    "      lemmatizer,\n",
    "      stopWords,\n",
    "      normalizer,\n",
    "      finisher\n",
    "])\n",
    "\n",
    "#We apply pipeline and recover lemmas as string\n",
    "projects = pipeline.fit(projects).transform(projects)\n",
    "\n",
    "udf_back2str = F.udf(lambda x:' '.join(list(x)), StringType() )\n",
    "projects = (\n",
    "    projects.withColumn(\"lemmas\",udf_back2str(F.col(\"finished_normalizedlemma\")))\n",
    "    .drop(\"rawtext\")\n",
    "    .drop(\"finished_normalizedlemma\")\n",
    ")\n",
    "\n",
    "#Show results of validation for n papers\n",
    "projects.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63e174",
   "metadata": {},
   "source": [
    "## 4. Save a table with `projectID` and `lemmas` to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4c5b911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.3 ms, sys: 3.68 ms, total: 36 ms\n",
      "Wall time: 50.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Save calculated lemmas to HDFS\n",
    "dir_parquet = Path(\"/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet\")\n",
    "\n",
    "projects.coalesce(40).write.parquet(\n",
    "    dir_parquet.joinpath(f\"projects_NLP.parquet\").as_posix(),\n",
    "    mode=\"overwrite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9416b",
   "metadata": {},
   "source": [
    "## 5. Optional: Check that the generated table looks OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf2a8e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:======================>                                 (16 + 0) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lemmatized projects: 61163\n",
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 689074                                                                                                                   \n",
      " lemmas    | holistic optimisation ship design operation life cycle maritime product typically associate large investment seldom b... \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 251681                                                                                                                   \n",
      " lemmas    | development nitric oxide releasing stent treatment coronary artery disease coronary artery disease one lead cause dea... \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 756059                                                                                                                   \n",
      " lemmas    | commercialising innovative disruptive solution improving human resources management practices happyornot global leade... \n",
      "-RECORD 3-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 799153                                                                                                                   \n",
      " lemmas    | insideout use storytelling understand politics exclusion europe south africa contemporary political narrative uk sout... \n",
      "-RECORD 4-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 738589                                                                                                                   \n",
      " lemmas    | highly accurate flexible robust scalable multicamera system spacecraft autonomous attitude determination low cost cam... \n",
      "-RECORD 5-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 254064                                                                                                                   \n",
      " lemmas    | modern approach career development smallgrain cereal breed carebreed project base multidisciplinary institute researc... \n",
      "-RECORD 6-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 604032                                                                                                                   \n",
      " lemmas    | mesosuperstructured hybrid solar cells project develop new class low cost solution processible hybrid solar cell term... \n",
      "-RECORD 7-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 271739                                                                                                                   \n",
      " lemmas    | rights liberties comparative perspective cross national analysis discrimination sexual minorities interdisciplinary r... \n",
      "-RECORD 8-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 841764                                                                                                                   \n",
      " lemmas    | strategic narrative nuclear order project investigate emergence projection contestation call strategic narrative nucl... \n",
      "-RECORD 9-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 607361                                                                                                                   \n",
      " lemmas    | advanced electric powertrain technology goal adept program produce virtual development environment epropulsion system... \n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 7.99 ms, sys: 220 Âµs, total: 8.21 ms\n",
      "Wall time: 3.84 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:=================================================>      (35 + 5) / 40]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Test that the saved table is correct\n",
    "projects = spark.read.parquet('/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet/projects_NLP.parquet')\n",
    "print('Number of lemmatized projects:', projects.count())\n",
    "projects.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94c4e59f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/opt/spark-3.1.1-bin-2.8.3/python/pyspark/sql/dataframe.py:1643\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m \n\u001b[1;32m   1635\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1644\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n\u001b[1;32m   1645\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "projects = spark.read.parquet('/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet/projects.parquet')\n",
    "lemmas = spark.read.parquet('/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet/projects_NLP.parquet')\n",
    "\n",
    "projects_lemmas = (projects.join(lemmas, projects.projectID ==  lemmas., \"right\")\n",
    "                      .drop(lemmas.id)\n",
    "                )\n",
    "\n",
    "print('Number of projects in joint table:', projects_lemmas.count())\n",
    "projects_lemmas.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9322a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Computer Science: 13654631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 00004ddfe8089303589fb12cddc05fefc7a0bd96                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | using static total causal ordering protocols achieve ordered view synchrony view synchronous communication vsc servic... \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 000051c2d8eff18654e5eaf3e636c02028ef96bb                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | author think paper highly cite                                                                                           \n",
      "-RECORD 2---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 00042289b5ddd6284a06dedf5272e4d27a2a5f6c                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | adaptive pulse shape cpofdm synchronization paper present new algorithm blind time offset estimation cyclic prefixort... \n",
      "-RECORD 3---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 000469a0fedfa15d33f8e0ab9f2d6f3309d7d45a                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | research integration capppdm based step xml deal product data exchange share capppdm integration network environmenti... \n",
      "-RECORD 4---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 000517d8f2b37864718111f832c6fb1cf1d1f79b                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | security vulnerability analysis correspond mitigation passwordbased authentication use offline personal authenticatio... \n",
      "-RECORD 5---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 0005db3f7cbffb7ce4d4dac237d273caa0eb4021                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | improving particle swarm optimization convergence controllable parameter particle swarm optimization algorithm value ... \n",
      "-RECORD 6---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 0006750da7d720e03fadd98dd20e014f9efab039                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | xquery interpreter                                                                                                       \n",
      "-RECORD 7---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 00070f1ddef29e150ceb6b2d8686541ba7ac0c4d                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | testbed network performance evaluation ipv4 ipv6 network layer protocol paper represent testbed perform measure evalu... \n",
      "-RECORD 8---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 0007a869cd502070f049d8779ee6bbfb06277283                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | improved density peaks clustering based sharedneighbors local cores manifold data sets novel cluster algorithm fast s... \n",
      "-RECORD 9---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 00080ed6324e584726730b53a4d592d5aa7cbcf5                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | microelectromechanical configuration optically reconfigurable gate array paper present proposal novel optically recon... \n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 201 ms, sys: 34.5 ms, total: 235 ms\n",
      "Wall time: 1min 41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Using SQL Expression\n",
    "papers_lemmas = papers_lemmas.filter(F.array_contains(\"fieldsOfStudy\", 'Computer Science'))\n",
    "print('Number of papers in Computer Science:', papers_lemmas.count())\n",
    "papers_lemmas.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667f4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
