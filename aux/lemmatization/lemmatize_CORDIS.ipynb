{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319e2032",
   "metadata": {},
   "source": [
    "# Lemmatize CORDIS projects using Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ccc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afef7c1-20f1-4e81-a7f9-013142b2e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/11 13:33:38 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:33:43 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:33:54 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:33:57 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:01 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:03 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:06 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:07 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:10 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:11 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:14 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:15 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:18 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:19 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:22 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:23 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:26 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:27 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:30 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:31 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:34 ERROR scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
      "java.util.ConcurrentModificationException\n",
      "\tat java.util.Hashtable$Enumerator.next(Hashtable.java:1387)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:424)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:420)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.mutable.MapLike.toSeq(MapLike.scala:75)\n",
      "\tat scala.collection.mutable.MapLike.toSeq$(MapLike.scala:72)\n",
      "\tat scala.collection.mutable.AbstractMap.toSeq(Map.scala:82)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.redactProperties(EventLoggingListener.scala:290)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.onJobStart(EventLoggingListener.scala:162)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/08/11 13:34:34 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:35 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:38 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:39 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:42 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:43 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:45 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:47 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:49 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:51 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:53 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:55 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:34:57 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:34:59 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:35:01 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:35:03 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:35:05 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:35:07 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "22/08/11 13:35:09 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 8.7 MiB\n",
      "22/08/11 13:35:11 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 13.1 MiB\n",
      "[Stage 63:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id     | 115843                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      " source | CORDIS                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      " bow    | (22797,[4,10,12,13,17,33,37,39,42,43,52,56,59,63,67,69,83,88,92,97,100,106,109,120,122,141,152,153,158,169,170,189,191,200,215,218,239,248,250,264,278,291,318,325,356,378,383,408,418,436,525,534,543,698,710,847,919,929,935,952,1077,1095,1113,1143,1364,1368,1449,1467,1629,1794,1820,1821,1853,1862,1932,2014,2067,2105,2158,2326,2722,2723,2817,2875,3042,3937,4188,4405,4509,4589,5055,5616,6447,6470,6561,6664,6723,7250,7289,8126,8615,9791,13680,14839,15109,16652,18298],[1.0,2.0,3.0,1.0,1.0,2.0,1.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,3.0,1.0,3.0,1.0,3.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,3.0,1.0,2.0,2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]) \n",
      "-RECORD 1------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id     | 115910                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      " source | CORDIS                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      " bow    | (22797,[2,5,13,14,19,20,22,23,26,27,31,33,37,39,53,55,56,57,66,70,74,75,78,96,100,106,119,122,124,132,134,183,210,222,245,266,270,281,292,327,330,385,418,480,488,507,509,539,540,592,624,635,645,651,668,694,700,711,747,754,784,802,804,805,825,890,948,1090,1094,1109,1236,1269,1295,1332,1499,1504,1702,1710,1747,1780,1788,1829,1979,1983,2513,2516,2795,2885,2906,2989,3443,3593,4195,4486,5140,5381,7250,8443,9233,9791,10000,11665,17771,22086],[1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,3.0,2.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,7.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                         \n",
      "-RECORD 2------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " id     | 200978                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      " source | CORDIS                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
      " bow    | (22797,[1,2,3,5,10,25,36,37,39,46,49,58,64,67,71,77,84,98,105,113,121,130,134,150,167,193,198,224,229,232,243,249,268,275,279,280,288,290,294,321,322,343,396,397,401,410,432,470,498,501,502,597,631,638,686,720,790,875,906,976,1005,1079,1103,1145,1193,1247,1255,1349,1374,1426,1535,1668,1773,1940,2422,2473,2930,3936,4215,4826,5776,6116,6157,7624,10635,11355],[1.0,1.0,2.0,6.0,1.0,2.0,1.0,1.0,2.0,1.0,2.0,1.0,1.0,3.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,3.0,2.0,1.0,1.0,1.0,1.0,1.0,2.0,5.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0,1.0,2.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,4.0,1.0,3.0,1.0,1.0,1.0,1.0,1.0,7.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                                                  \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA as pysparkLDA\n",
    "\n",
    "corpusFile = Path(\"/export/usuarios_ml4ds/jarenas/github/IntelComp/ITMT/topicmodeler/testproject/TMmodels/sparkkk/corpus.parquet\")\n",
    "df = spark.read.parquet(f\"file://{corpusFile.resolve().as_posix()}\")\n",
    "lda = pysparkLDA(featuresCol=\"bow\", maxIter=20, k=25,\n",
    "            optimizer='online', subsamplingRate=0.05,\n",
    "            optimizeDocConcentration=True,\n",
    "            docConcentration=25*[0.2])\n",
    "ldaModel = lda.fit(df)\n",
    "\n",
    "df.show(n=3, vertical=True, truncate=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf8691e2-8753-4fc5-8109-4a1fc2a24291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/11 15:35:07 ERROR scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
      "java.util.ConcurrentModificationException\n",
      "\tat java.util.Hashtable$Enumerator.next(Hashtable.java:1387)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:424)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:420)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.mutable.MapLike.toSeq(MapLike.scala:75)\n",
      "\tat scala.collection.mutable.MapLike.toSeq$(MapLike.scala:72)\n",
      "\tat scala.collection.mutable.AbstractMap.toSeq(Map.scala:82)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.redactProperties(EventLoggingListener.scala:290)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.onJobStart(EventLoggingListener.scala:162)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/08/11 15:35:07 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 4.4 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60596, 25)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "import numpy as np\n",
    "\n",
    "df = spark.read.parquet(f\"file://{corpusFile.resolve().as_posix()}\")\n",
    "df = ldaModel.transform(df).select(\"topicDistribution\").withColumn('topicDistribution', vector_to_array('topicDistribution'))\n",
    "thetas32 = np.array([el[0] for el in df.toPandas().values.tolist()])\n",
    "thetas32.sum(axis=1)\n",
    "print(thetas32.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ee35c20-ac81-460c-9af8-10d744158c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " topic       | 0                                                                                                                                                                                                                                 \n",
      " termIndices | [4, 3, 52, 65, 31, 17, 27, 8, 43, 47]                                                                                                                                                                                             \n",
      " termWeights | [0.010632095659496191, 0.007795066999781101, 0.006849610171850877, 0.006711525464863618, 0.0061415744490459015, 0.005932557592324106, 0.005390082172262677, 0.005039874745339203, 0.00489998538561223, 0.004594162372837045]      \n",
      "-RECORD 1----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " topic       | 1                                                                                                                                                                                                                                 \n",
      " termIndices | [5766, 2, 3942, 1254, 1, 6, 8, 8010, 0, 7]                                                                                                                                                                                        \n",
      " termWeights | [0.0023037507459389987, 0.0016053274004690472, 0.0014908269201708995, 0.0014804304382645694, 0.001356404923348987, 0.0010747975143558443, 9.437664475314392E-4, 9.427352583661931E-4, 8.731525075485076E-4, 8.288579221810217E-4] \n",
      "-RECORD 2----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " topic       | 2                                                                                                                                                                                                                                 \n",
      " termIndices | [1457, 328, 773, 1346, 3590, 3, 32, 3372, 8, 129]                                                                                                                                                                                 \n",
      " termWeights | [0.010817371276213138, 0.008543793761198544, 0.004671940375398029, 0.00399753738818423, 0.0028444735738398365, 0.002674027357308922, 0.0023595125122077816, 0.002234597961762956, 0.002083986053479082, 0.0019421665560886792]    \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ldaModel.describeTopics().show(n=3, vertical=True, truncate=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c533377-1527-4d3d-b94b-7d30cb130ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00261458 0.00297528 0.00015629 0.00779507 0.0106321  0.0030569\n",
      " 0.00148275 0.00041544 0.00503987 0.00031139]\n",
      "(25, 22797)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "betas = normalize(ldaModel.topicsMatrix().toArray().T, axis=1, norm='l1')\n",
    "print(betas[0,:10])\n",
    "print(betas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ee1f5",
   "metadata": {},
   "source": [
    "## 1. Read project information and concatenate the `title` and `objective` fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f6fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/08/10 14:11:21 WARN conf.HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "22/08/10 14:11:21 WARN conf.HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "22/08/10 14:11:24 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "22/08/10 14:11:24 WARN metastore.ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore jarenas@192.168.148.225\n",
      "22/08/10 14:11:24 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "22/08/10 14:11:24 WARN metastore.ObjectStore: Failed to get database parquet, returning NoSuchObjectException\n",
      "[Stage 2:=====================================================>(996 + 4) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects before language filtering: 61163\n",
      "CPU times: user 25.7 ms, sys: 5.95 ms, total: 31.7 ms\n",
      "Wall time: 16.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Loading papers table text fields, and concatenating them for lemmatization\n",
    "projects = spark.sql(\"SELECT projectID, title, objective FROM parquet.`/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet/projects.parquet`\")\n",
    "#projects = spark.read.parquet('/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet/projects.parquet')\n",
    "projects = projects.repartition(numPartitions=1000)\n",
    "\n",
    "#Concatenate text fields to lemmatize\n",
    "projects = (\n",
    "    projects.withColumn(\"rawtext\",F.concat_ws('. ', \"title\", \"objective\"))\n",
    "    .select(\"projectID\",\"rawtext\")\n",
    ")\n",
    "\n",
    "print('Number of projects before language filtering:', projects.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba78d8",
   "metadata": {},
   "source": [
    "## 2. Filter project abstracts that are not in English Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2ecc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "[ | ]ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==============>                                           (3 + 0) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:==============>                                           (3 + 0) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 14:12:07.355070: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-10 14:12:07.425992: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==========================================>          (807 + 40) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects in English: 60626\n",
      "CPU times: user 68.7 ms, sys: 17.1 ms, total: 85.8 ms\n",
      "Wall time: 54.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Pipeline for language detection\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"rawtext\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "languageDetector = LanguageDetectorDL.pretrained() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"language\")\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "      documentAssembler,\n",
    "      languageDetector\n",
    "    ])\n",
    "\n",
    "#Apply language detection pipeline\n",
    "projects = pipeline.fit(projects).transform(projects)\n",
    "projects = (\n",
    "    projects.filter(F.col(\"language.result\")[0]==\"en\")\n",
    "    .drop(\"language\")\n",
    ")\n",
    "\n",
    "print('Number of projects in English:', projects.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d59da",
   "metadata": {},
   "source": [
    "## 3. Define and Run Lemmatization Pipeline\n",
    "\n",
    "   - We work on documents created in Subsection 2\n",
    "   - Sentence Detection and Tokenizer applied to detect tokens\n",
    "   - Lemmatization is carried out\n",
    "   - Stopwords are applied\n",
    "   - Punctuation symbols are removed\n",
    "   - Result is converted back from Spark NLP annotations to string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eecf0f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:===================================================>    (11 + 0) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ — ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 963941                                                                                                                   \n",
      " rawtext   | Acquiring assembly skills by robot learning. Present-day industrial robots are made for the purpose of repeating seve... \n",
      " lemmas    | acquiring assembly skill robot learn presentday industrial robot make purpose repeat several task thousands time them... \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 268351                                                                                                                   \n",
      " rawtext   | Manycore Application Development and Modeling Environment. The multicore revolution (parallel computing on a chip) is... \n",
      " lemmas    | manycore application development modeling environment multicore revolution parallel computing chip predict many major... \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 301045                                                                                                                   \n",
      " rawtext   | Role of the protein kinase Mastl in Cell Division and Cancer. The cell division cycle is an essential step in living ... \n",
      " lemmas    | role protein kinase mastl cell division cancer cell division cycle essential step live organism different study lead ... \n",
      "-RECORD 3-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 101020692                                                                                                                \n",
      " rawtext   | The Cinematic Battle for the Adriatic: Films, Frontiers, and the Trieste Crisis. The project will analyse cinematic p... \n",
      " lemmas    | cinematic battle adriatic films frontiers trieste crisis project analyse cinematic practice relate trieste crisis 194... \n",
      "-RECORD 4-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 824988                                                                                                                   \n",
      " rawtext   | Machine learning to augment shared knowledge in federated privacy-preserving scenarios. The massive increase in data ... \n",
      " lemmas    | machine learn augment share knowledge federated privacypreserving scenario massive increase data collect store worldw... \n",
      "-RECORD 5-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 736254                                                                                                                   \n",
      " rawtext   | Innovative processing plant for optimal production of Decolourised Hydrolysed Protein (DHP). A secure, cost-effective... \n",
      " lemmas    | innovative process plant optimal production decolourised hydrolysed protein dhp secure costeffective ecofriendly bloo... \n",
      "-RECORD 6-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 760809                                                                                                                   \n",
      " rawtext   | E2E-aware Optimizations and advancements for the Network Edge of 5G New Radio. ONE5G commits to provide technical inv... \n",
      " lemmas    | e2eaware optimizations advancement network edge 5g new radio one5g commit provide technical investigation recommendat... \n",
      "-RECORD 7-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 676036                                                                                                                   \n",
      " rawtext   | STARs that 'R' Young : When do stars form in clustered environments?. STARRY (STARs that ‘R’ Young) is a twin site EI... \n",
      " lemmas    | stars r young star form cluster environment starry stars r young twin site eid provide training 2 inexperienced resea... \n",
      "-RECORD 8-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 217429                                                                                                                   \n",
      " rawtext   | Identification of Priority RESearch TOpics for SME associations in the construction sector with a focus on new techno... \n",
      " lemmas    | identification priority research topics sme association construction sector focus new technology energy ict new mater... \n",
      "-RECORD 9-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 641386                                                                                                                   \n",
      " rawtext   | Minimal invasive x-ray source enabled by a novel vacuum packaging technique. In this project we aim to demonstrate an... \n",
      " lemmas    | minimal invasive xray source enable novel vacuum package technique project aim demonstrate test memsbased miniaturize... \n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 100 ms, sys: 44.6 ms, total: 145 ms\n",
      "Wall time: 19.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#Next, we carry out the lemmatization pipeline\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "stopWords = StopWordsCleaner() \\\n",
    "    .setInputCols([\"lemma\"]) \\\n",
    "    .setOutputCol(\"cleanlemma\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"cleanlemma\"]) \\\n",
    "    .setOutputCol(\"normalizedlemma\") \\\n",
    "    .setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"])\n",
    "\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['normalizedlemma'])\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "#      documentAssembler,\n",
    "      sentenceDetector,\n",
    "      tokenizer,\n",
    "      lemmatizer,\n",
    "      stopWords,\n",
    "      normalizer,\n",
    "      finisher\n",
    "])\n",
    "\n",
    "#We apply pipeline and recover lemmas as string\n",
    "projects = pipeline.fit(projects).transform(projects)\n",
    "\n",
    "udf_back2str = F.udf(lambda x:' '.join(list(x)), StringType() )\n",
    "projects = (\n",
    "    projects.withColumn(\"lemmas\",udf_back2str(F.col(\"finished_normalizedlemma\")))\n",
    "    .drop(\"finished_normalizedlemma\")\n",
    ")\n",
    "\n",
    "#Show results of validation for n papers\n",
    "projects.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63e174",
   "metadata": {},
   "source": [
    "## 4. Save a table with `projectID`, `rawtext` and `lemmas` to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4c5b911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.4 ms, sys: 2.32 ms, total: 57.8 ms\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Save calculated lemmas to HDFS\n",
    "dir_parquet = Path(\"/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet\")\n",
    "\n",
    "projects.coalesce(40).write.parquet(\n",
    "    dir_parquet.joinpath(f\"projects_NLP.parquet\").as_posix(),\n",
    "    mode=\"overwrite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9416b",
   "metadata": {},
   "source": [
    "## 5. Optional: Check that the generated table looks OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf2a8e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:======================>                                 (16 + 0) / 40]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lemmatized projects: 61163\n",
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 689074                                                                                                                   \n",
      " lemmas    | holistic optimisation ship design operation life cycle maritime product typically associate large investment seldom b... \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 251681                                                                                                                   \n",
      " lemmas    | development nitric oxide releasing stent treatment coronary artery disease coronary artery disease one lead cause dea... \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 756059                                                                                                                   \n",
      " lemmas    | commercialising innovative disruptive solution improving human resources management practices happyornot global leade... \n",
      "-RECORD 3-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 799153                                                                                                                   \n",
      " lemmas    | insideout use storytelling understand politics exclusion europe south africa contemporary political narrative uk sout... \n",
      "-RECORD 4-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 738589                                                                                                                   \n",
      " lemmas    | highly accurate flexible robust scalable multicamera system spacecraft autonomous attitude determination low cost cam... \n",
      "-RECORD 5-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 254064                                                                                                                   \n",
      " lemmas    | modern approach career development smallgrain cereal breed carebreed project base multidisciplinary institute researc... \n",
      "-RECORD 6-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 604032                                                                                                                   \n",
      " lemmas    | mesosuperstructured hybrid solar cells project develop new class low cost solution processible hybrid solar cell term... \n",
      "-RECORD 7-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 271739                                                                                                                   \n",
      " lemmas    | rights liberties comparative perspective cross national analysis discrimination sexual minorities interdisciplinary r... \n",
      "-RECORD 8-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 841764                                                                                                                   \n",
      " lemmas    | strategic narrative nuclear order project investigate emergence projection contestation call strategic narrative nucl... \n",
      "-RECORD 9-----------------------------------------------------------------------------------------------------------------------------\n",
      " projectID | 607361                                                                                                                   \n",
      " lemmas    | advanced electric powertrain technology goal adept program produce virtual development environment epropulsion system... \n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 7.99 ms, sys: 220 µs, total: 8.21 ms\n",
      "Wall time: 3.84 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Test that the saved table is correct\n",
    "projects = spark.read.parquet('/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet/projects_NLP.parquet')\n",
    "print('Number of lemmatized projects:', projects.count())\n",
    "projects.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94c4e59f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/opt/spark-3.1.1-bin-2.8.3/python/pyspark/sql/dataframe.py:1643\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m \n\u001b[1;32m   1635\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1644\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n\u001b[1;32m   1645\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "projects = spark.read.parquet('/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet/projects.parquet')\n",
    "lemmas = spark.read.parquet('/export/ml4ds/IntelComp/Datalake/CORDIS/20220221/new_parquet/projects_NLP.parquet')\n",
    "\n",
    "projects_lemmas = (projects.join(lemmas, projects.projectID ==  lemmas., \"right\")\n",
    "                      .drop(lemmas.id)\n",
    "                )\n",
    "\n",
    "print('Number of projects in joint table:', projects_lemmas.count())\n",
    "projects_lemmas.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9322a5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers in Computer Science: 13654631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 00004ddfe8089303589fb12cddc05fefc7a0bd96                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | using static total causal ordering protocols achieve ordered view synchrony view synchronous communication vsc servic... \n",
      "-RECORD 1---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 000051c2d8eff18654e5eaf3e636c02028ef96bb                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | author think paper highly cite                                                                                           \n",
      "-RECORD 2---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 00042289b5ddd6284a06dedf5272e4d27a2a5f6c                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | adaptive pulse shape cpofdm synchronization paper present new algorithm blind time offset estimation cyclic prefixort... \n",
      "-RECORD 3---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 000469a0fedfa15d33f8e0ab9f2d6f3309d7d45a                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | research integration capppdm based step xml deal product data exchange share capppdm integration network environmenti... \n",
      "-RECORD 4---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 000517d8f2b37864718111f832c6fb1cf1d1f79b                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | security vulnerability analysis correspond mitigation passwordbased authentication use offline personal authenticatio... \n",
      "-RECORD 5---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 0005db3f7cbffb7ce4d4dac237d273caa0eb4021                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | improving particle swarm optimization convergence controllable parameter particle swarm optimization algorithm value ... \n",
      "-RECORD 6---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 0006750da7d720e03fadd98dd20e014f9efab039                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | xquery interpreter                                                                                                       \n",
      "-RECORD 7---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 00070f1ddef29e150ceb6b2d8686541ba7ac0c4d                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | testbed network performance evaluation ipv4 ipv6 network layer protocol paper represent testbed perform measure evalu... \n",
      "-RECORD 8---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 0007a869cd502070f049d8779ee6bbfb06277283                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | improved density peaks clustering based sharedneighbors local cores manifold data sets novel cluster algorithm fast s... \n",
      "-RECORD 9---------------------------------------------------------------------------------------------------------------------------------\n",
      " id            | 00080ed6324e584726730b53a4d592d5aa7cbcf5                                                                                 \n",
      " fieldsOfStudy | [Computer Science]                                                                                                       \n",
      " lemmas        | microelectromechanical configuration optically reconfigurable gate array paper present proposal novel optically recon... \n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 201 ms, sys: 34.5 ms, total: 235 ms\n",
      "Wall time: 1min 41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Using SQL Expression\n",
    "papers_lemmas = papers_lemmas.filter(F.array_contains(\"fieldsOfStudy\", 'Computer Science'))\n",
    "print('Number of papers in Computer Science:', papers_lemmas.count())\n",
    "papers_lemmas.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667f4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
