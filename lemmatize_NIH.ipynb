{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319e2032",
   "metadata": {},
   "source": [
    "# Lemmatize NIH project abstracts using Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58ccc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "import sparknlp\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ee1f5",
   "metadata": {},
   "source": [
    "## 1. Read projects and abstracts. Concatenate text fields\n",
    "\n",
    "**Important:** We will use Pandas to parse the files, because pySpark does not seem to process NIH CSV files correctly when there are additional '\"' in some of the files. Pandas seems to do it OK.\n",
    "\n",
    "We will also do some basic cleanup of the text fields. This is directly done in Pandas, it may be better to do it in Spark\n",
    "\n",
    "Result is saved in a CSV file in the local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f380bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIHprojects = pd.read_csv('file:///export/data_ml4ds/IntelComp/Datasets/nih/csv/20220119/projects.csv', low_memory=False)\n",
    "NIHabstracts = pd.read_csv('file:///export/data_ml4ds/IntelComp/Datasets/nih/csv/20220119/abstracts.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a7646c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIHprojects = NIHprojects[[\"APPLICATION_ID\", \"PROJECT_TITLE\", \"PROJECT_TERMS\", \"PHR\"]]\n",
    "NIHprojects = NIHprojects.fillna(\"\")\n",
    "NIHprojects[\"PROJECT_TERMS\"] = NIHprojects[\"PROJECT_TERMS\"].apply(lambda x: x.replace(\";\", \"; \"))\n",
    "NIHprojects[\"PHR\"] = (NIHprojects[\"PHR\"]\n",
    "                          .apply(str)\n",
    "                          .replace(\"PROJECT NARRATIVE \", \"\")\n",
    "                          .replace('\"', '')\n",
    "                          .apply(lambda x: \" \".join(x.split()))\n",
    "                     )\n",
    "NIHabstracts = NIHabstracts[[\"APPLICATION_ID\", \"ABSTRACT_TEXT\"]]\n",
    "NIHabstracts[\"ABSTRACT_TEXT\"] = (NIHabstracts[\"ABSTRACT_TEXT\"]\n",
    "                                    .apply(str)\n",
    "                                    .replace(\"PROJECT SUMMARY/ABSTRACT\", \"\")\n",
    "                                    .replace(\"PROJECT SUMMARY\", \"\")\n",
    "                                    .replace('\"', '')\n",
    "                                    .apply(lambda x: \" \".join(x.split()))\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47267ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIHprojects = NIHprojects.merge(NIHabstracts, how=\"inner\", on=\"APPLICATION_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad6ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "NIHprojects[\"rawtext\"] = NIHprojects[\"PROJECT_TITLE\"] + \". \" + \\\n",
    "                         NIHprojects[\"ABSTRACT_TEXT\"] + \". \" + \\\n",
    "                         NIHprojects[\"PHR\"] + \". \" + \\\n",
    "                         NIHprojects[\"PROJECT_TERMS\"]\n",
    "NIHprojects = NIHprojects[[\"APPLICATION_ID\", \"rawtext\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65b3588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe will be saved to temporary file\n",
    "NIHprojects.to_csv(\"./NIHrawtext.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba78d8",
   "metadata": {},
   "source": [
    "## 2. Filter abstracts that are not in English Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70aa712e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 19:57:59 ERROR scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
      "java.util.ConcurrentModificationException\n",
      "\tat java.util.Hashtable$Enumerator.next(Hashtable.java:1387)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:424)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:420)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.mutable.MapLike.toSeq(MapLike.scala:75)\n",
      "\tat scala.collection.mutable.MapLike.toSeq$(MapLike.scala:72)\n",
      "\tat scala.collection.mutable.AbstractMap.toSeq(Map.scala:82)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.redactProperties(EventLoggingListener.scala:290)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.onJobStart(EventLoggingListener.scala:162)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "[Stage 17:===================================================>(1993 + 7) / 2000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects before language filtering: 2278732\n",
      "CPU times: user 36.7 ms, sys: 3.49 ms, total: 40.2 ms\n",
      "Wall time: 30.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "NIHraw = spark.read.csv(\"file:///export/usuarios_ml4ds/jarenas/github/IntelComp/ITMT/topicmodeler/NIHrawtext.csv\", header=True)\n",
    "NIHraw = NIHraw.repartition(2000)\n",
    "print(\"Number of projects before language filtering:\", NIHraw.count())\n",
    "#NIHraw.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba2ecc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld_wiki_tatoeba_cnn_21 download started this may take some time.\n",
      "Approximate size to download 7.1 MB\n",
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/03 19:59:13 ERROR scheduler.TaskSchedulerImpl: Lost executor 18 on node62.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:13 WARN scheduler.TaskSetManager: Lost task 12.0 in stage 19.0 (TID 2281) (node62.cluster.tsc.uc3m.es executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:13 WARN scheduler.TaskSetManager: Lost task 2.0 in stage 19.0 (TID 2271) (node62.cluster.tsc.uc3m.es executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:13 WARN scheduler.TaskSetManager: Lost task 32.0 in stage 19.0 (TID 2301) (node62.cluster.tsc.uc3m.es executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:13 WARN scheduler.TaskSetManager: Lost task 22.0 in stage 19.0 (TID 2291) (node62.cluster.tsc.uc3m.es executor 18): ExecutorLostFailure (executor 18 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:37 ERROR scheduler.TaskSchedulerImpl: Lost executor 19 on node43.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:37 WARN scheduler.TaskSetManager: Lost task 2.1 in stage 19.0 (TID 2311) (node43.cluster.tsc.uc3m.es executor 19): ExecutorLostFailure (executor 19 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:37 WARN scheduler.TaskSetManager: Lost task 32.1 in stage 19.0 (TID 2310) (node43.cluster.tsc.uc3m.es executor 19): ExecutorLostFailure (executor 19 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:37 WARN scheduler.TaskSetManager: Lost task 12.1 in stage 19.0 (TID 2312) (node43.cluster.tsc.uc3m.es executor 19): ExecutorLostFailure (executor 19 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:37 WARN scheduler.TaskSetManager: Lost task 22.1 in stage 19.0 (TID 2309) (node43.cluster.tsc.uc3m.es executor 19): ExecutorLostFailure (executor 19 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:56 ERROR scheduler.TaskSchedulerImpl: Lost executor 20 on node71.cluster.tsc.uc3m.es: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:56 WARN scheduler.TaskSetManager: Lost task 12.2 in stage 19.0 (TID 2314) (node71.cluster.tsc.uc3m.es executor 20): ExecutorLostFailure (executor 20 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:56 WARN scheduler.TaskSetManager: Lost task 2.2 in stage 19.0 (TID 2316) (node71.cluster.tsc.uc3m.es executor 20): ExecutorLostFailure (executor 20 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:56 WARN scheduler.TaskSetManager: Lost task 22.2 in stage 19.0 (TID 2313) (node71.cluster.tsc.uc3m.es executor 20): ExecutorLostFailure (executor 20 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/03 19:59:56 WARN scheduler.TaskSetManager: Lost task 32.2 in stage 19.0 (TID 2315) (node71.cluster.tsc.uc3m.es executor 20): ExecutorLostFailure (executor 20 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "[Stage 20:===============================================>   (1851 + 41) / 2000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of projects in English: 2231208\n",
      "CPU times: user 203 ms, sys: 31.6 ms, total: 235 ms\n",
      "Wall time: 16min 58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Pipeline for language detection\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"rawtext\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "languageDetector = LanguageDetectorDL.pretrained() \\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"language\")\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "      documentAssembler,\n",
    "      languageDetector\n",
    "    ])\n",
    "\n",
    "#Apply language detection pipeline\n",
    "NIHraw = pipeline.fit(NIHraw).transform(NIHraw)\n",
    "NIHraw = (\n",
    "    NIHraw.filter(F.col(\"language.result\")[0]==\"en\")\n",
    "    .drop(\"language\")\n",
    ")\n",
    "\n",
    "print('Number of projects in English:', NIHraw.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d59da",
   "metadata": {},
   "source": [
    "## 3. Define and Run Lemmatization Pipeline\n",
    "\n",
    "   - We work on documents created in Subsection 2\n",
    "   - Sentence Detection and Tokenizer applied to detect tokens\n",
    "   - Lemmatization is carried out\n",
    "   - Stopwords are applied\n",
    "   - Punctuation symbols are removed\n",
    "   - Result is converted back from Spark NLP annotations to string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eecf0f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ | ]lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[ / ]Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:===================>                                     (4 + 0) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 6930018                                                                                                                  \n",
      " lemmas         | il17 mediated muc gene expression airway epithelium description provide applicant mucus hypersecretion persistent air... \n",
      "-RECORD 1----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 6878069                                                                                                                  \n",
      " lemmas         | maturation cognitiverelated brain potentials description provide applicant axiomatic developmental increase performan... \n",
      "-RECORD 2----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 6908914                                                                                                                  \n",
      " lemmas         | chemokines lymphocyte trafficking cns description provide applicant lymphocytes predominant leukocyte find central ne... \n",
      "-RECORD 3----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 6887718                                                                                                                  \n",
      " lemmas         | causal reasoning children adults unreadable description provide applicant developmental research show profound change... \n",
      "-RECORD 4----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 6883854                                                                                                                  \n",
      " lemmas         | role genetic modulation ppargamma phosphoryla ppary play critical role adipocyte differentiation glucose metabolism t... \n",
      "-RECORD 5----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 6897465                                                                                                                  \n",
      " lemmas         | molecular architecture naglucose cotransporter unreadable description provide applicant naglucose cotransporter sglt1... \n",
      "-RECORD 6----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 6866485                                                                                                                  \n",
      " lemmas         | zinc magnesium nmr biological systems unreadable description provide applicant propose research represent novel appli... \n",
      "-RECORD 7----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 7006822                                                                                                                  \n",
      " lemmas         | consensus genebased immunogens rhesus monkeys extreme genetic diversity hiv1 isolate pose major challenge development... \n",
      "-RECORD 8----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 6944162                                                                                                                  \n",
      " lemmas         | coretransgenics baderc transgenic mouse core service facility utilize investigatorderived dna construct create founde... \n",
      "-RECORD 9----------------------------------------------------------------------------------------------------------------------------------\n",
      " APPLICATION_ID | 6986456                                                                                                                  \n",
      " lemmas         | satscan spatial scan statistic surveillance software satscan free software product geographical cluster detection inf... \n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 296 ms, sys: 34.1 ms, total: 330 ms\n",
      "Wall time: 17min 34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#Next, we carry out the lemmatization pipeline\n",
    "\n",
    "sentenceDetector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"lemma\")\n",
    "\n",
    "stopWords = StopWordsCleaner() \\\n",
    "    .setInputCols([\"lemma\"]) \\\n",
    "    .setOutputCol(\"cleanlemma\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"cleanlemma\"]) \\\n",
    "    .setOutputCol(\"normalizedlemma\") \\\n",
    "    .setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"])\n",
    "\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['normalizedlemma'])\n",
    "\n",
    "pipeline = Pipeline() \\\n",
    "    .setStages([\n",
    "      sentenceDetector,\n",
    "      tokenizer,\n",
    "      lemmatizer,\n",
    "      stopWords,\n",
    "      normalizer,\n",
    "      finisher\n",
    "])\n",
    "\n",
    "#We apply pipeline and recover lemmas as string\n",
    "NIHraw = pipeline.fit(NIHraw).transform(NIHraw)\n",
    "\n",
    "udf_back2str = F.udf(lambda x:' '.join(list(x)), StringType() )\n",
    "NIHraw = (\n",
    "    NIHraw.withColumn(\"lemmas\",udf_back2str(F.col(\"finished_normalizedlemma\")))\n",
    "    .drop(\"rawtext\")\n",
    "    .drop(\"finished_normalizedlemma\")\n",
    ")\n",
    "\n",
    "#Show results of validation for n papers\n",
    "NIHraw.show(n=10, truncate=120, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63e174",
   "metadata": {},
   "source": [
    "## 4. Save a table with `id` and `lemmas` to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4c5b911",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 20:50:47,263:9854(0x7f7d74add640):ZOO_ERROR@handle_socket_error_msg@1782: Socket [10.0.12.18:2181] zk retcode=-4, errno=112(Host is down): failed while receiving a server response\n",
      "I0403 20:50:47.264180 10638 group.cpp:452] Lost connection to ZooKeeper, attempting to reconnect ...\n",
      "2022-04-03 20:50:47,270:9854(0x7f7d74add640):ZOO_INFO@check_events@1764: initiated connection to server [10.0.12.60:2181]\n",
      "2022-04-03 20:50:47,313:9854(0x7f7d74add640):ZOO_ERROR@handle_socket_error_msg@1782: Socket [10.0.12.60:2181] zk retcode=-4, errno=112(Host is down): failed while receiving a server response\n",
      "2022-04-03 20:50:47,313:9854(0x7f7d74add640):ZOO_INFO@check_events@1764: initiated connection to server [10.0.12.76:2181]\n",
      "2022-04-03 20:50:47,314:9854(0x7f7d74add640):ZOO_ERROR@handle_socket_error_msg@1782: Socket [10.0.12.76:2181] zk retcode=-4, errno=112(Host is down): failed while receiving a server response\n",
      "2022-04-03 20:50:47,314:9854(0x7f7d74add640):ZOO_INFO@check_events@1764: initiated connection to server [10.0.12.77:2181]\n",
      "2022-04-03 20:50:47,315:9854(0x7f7d74add640):ZOO_ERROR@handle_socket_error_msg@1782: Socket [10.0.12.77:2181] zk retcode=-4, errno=112(Host is down): failed while receiving a server response\n",
      "2022-04-03 20:50:47,315:9854(0x7f7d74add640):ZOO_INFO@check_events@1764: initiated connection to server [10.0.12.78:2181]\n",
      "2022-04-03 20:50:47,316:9854(0x7f7d74add640):ZOO_ERROR@handle_socket_error_msg@1782: Socket [10.0.12.78:2181] zk retcode=-4, errno=112(Host is down): failed while receiving a server response\n",
      "2022-04-03 20:50:47,328:9854(0x7f7d74add640):ZOO_INFO@check_events@1764: initiated connection to server [10.0.12.51:2181]\n",
      "2022-04-03 20:50:47,549:9854(0x7f7d74add640):ZOO_ERROR@handle_socket_error_msg@1782: Socket [10.0.12.51:2181] zk retcode=-4, errno=112(Host is down): failed while receiving a server response\n",
      "2022-04-03 20:50:47,549:9854(0x7f7d74add640):ZOO_INFO@check_events@1764: initiated connection to server [10.0.12.75:2181]\n",
      "2022-04-03 20:50:47,550:9854(0x7f7d74add640):ZOO_ERROR@handle_socket_error_msg@1782: Socket [10.0.12.75:2181] zk retcode=-4, errno=112(Host is down): failed while receiving a server response\n",
      "2022-04-03 20:50:50,886:9854(0x7f7d74add640):ZOO_INFO@check_events@1764: initiated connection to server [10.0.12.18:2181]\n",
      "2022-04-03 20:50:50,890:9854(0x7f7d74add640):ZOO_INFO@check_events@1811: session establishment complete on server [10.0.12.18:2181], sessionId=0x1000002e9860399, negotiated timeout=10000\n",
      "I0403 20:50:50.890461 10608 group.cpp:341] Group process (zookeeper-group(1)@192.168.148.225:45937) reconnected to ZooKeeper\n",
      "I0403 20:50:50.890487 10608 group.cpp:831] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 830 ms, sys: 220 ms, total: 1.05 s\n",
      "Wall time: 30min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Save calculated lemmas to HDFS\n",
    "dir_parquet = Path(\"/export/ml4ds/IntelComp/Datalake/NIH/20220119\")\n",
    "\n",
    "NIHraw.coalesce(1000).write.parquet(\n",
    "    dir_parquet.joinpath(f\"NIH_NLP.parquet\").as_posix(),\n",
    "    mode=\"overwrite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a95acf",
   "metadata": {},
   "source": [
    "## 5. Delete temporary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c19b8fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file deleted\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "file = \"./NIHrawtext.csv\"\n",
    "\n",
    "if(os.path.exists(file) and os.path.isfile(file)):\n",
    "    os.remove(file)\n",
    "    print(\"file deleted\")\n",
    "else:\n",
    "    print(\"file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1d6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
